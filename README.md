# advanced-statistical-learning-F17
This repository contains much of my coursework from Advanced Statistical Learning (ST697) at UA.

#### Game Theory & Boosting (Final Project):
Abstract: Weight update algorithms have been both derived and applied across disciplines. The classic game-theoretical problem of finding a Nash Equilibrium in a zero-sum, two-player game can be approximately solved using a multiplicative weight update algorithm. The concept of boosting can be understood as solving a game whose two players are a set of hypotheses (or weak classifiers) and a set of examples. In boosting by sampling, the example set has an adversarial relationship with the hypothesis, in that it is constructed so as to maximally challenge the hypothesis. Solving a game against a player of unknown hostility (maybe adversarial, maybe not) is of interest as well; the “learning” player may want to be able to take advantage of any game value their opponent may be willing to give up. This problem is approached with the tools of on-line prediction.

#### binary classification data generation:
Generates a toy data set for binary classification based on a plot (p. 17) in Hastie's 'Elements of Statistical Learning.'

#### linear regression vs KNN: 
Comparison of linear regression and KNN for classification of handwritten digits 2 & 3.

#### ridge regression prediction:
A ridge regression (weight decay) model to predict the price of real estate.

#### parameter tuning for glm:
Compares glm performance for 2 different choices of lambda (coef penalty). (1se rule)

#### elastic net function:
Elastic net function using coordinate descent, plus a test example.

#### logistic regression function:
A function for logistic regression, plus a test example.

#### log density ratio estimation:
Estimate log density ratio for Gaussian, kernel density, non-parametric, and Bayesian models.
