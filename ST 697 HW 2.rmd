---
title: "ST 697 HW 2"
author: "Jenna Losh"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### 2.1

a.
$$\sum_{i=1}^nz_{ij}  = s_j \left(\sum_{i=1}^nx_{ij}-\sum_{i=1}^n\bar{x}\right) = s_j\left(\sum_{i=1}^nx_{ij}-n\bar{x}\right)$$
$$s_j\left(\sum_{i=1}^nx_{ij}-\sum_{i=1}^nx_{ij}\right)=0$$ 
$$\sum_{i=1}^nz_{ij}^2  = \sum_{i=1}^n \left( \frac{x_{ij}-\bar{x}}{\sum_{i=1}^nx_{ij}-\bar{x}}\right)^2 =  \left( \frac{\sum_{i=1}^nx_{ij}-\bar{x}}{\sum_{i=1}^nx_{ij}-\bar{x}}\right)^2 = 1$$

b.
```{r}
x = matrix(1:20, 5, 4)
sj = sqrt(sum(sweep(x, 2, colMeans(x), '-')^2))
zij = (sweep(x, 2, colMeans(x), '-'))/sj
sum(zij)
sum(zij^2)

```

c.
$$\hat{b} = \underset{b\in \mathbb{R}^{p+1}}{\mathrm{argmin}}\sum_{i=1}^n \left( y_{i} - b_{0} - \sum_{j=1}^pz_{ij}b_{j}\right)^2 =  $$
$$\underset{b\in \mathbb{R}^{p+1}}{\mathrm{argmin}}\sum_{i=1}^n \left( y_{i} - b_{0} - \left(\sum_{j=1}^p\frac{x_{ij}b_{j}}{s_{j}} + \sum_{j=1}^p\frac{\bar{x}_{j}b_{j}}{s_{j}} \right)  \right)^2 $$
The final term in this formula is a constant, and can be combined with the intercept $b_{0}$, leaving
$$\hat{b} =\underset{b\in \mathbb{R}^{p+1}}{\mathrm{argmin}}\sum_{i=1}^n \left( y_{i} - b_{0} - \sum_{j=1}^p\frac{x_{ij}b_{j}}{s_{j}}\right)^2$$

d. Referring back to our definition of $\hat\beta$, we can see that by letting $\hat\beta_{0} = \hat{b}_{0}$, we are able to make the connection that 
$$\sum_{j=1}^px_{ij}\beta_{j}=\sum_{j=1}^px_{ij}\frac{b_{j}}{s_{j}}$$ 
So,
$$\beta_{j} =\frac{b_j}{s_j}$$ 

e. When we use the alternative predictors, we saved one degree of freedom when we eliminated the final term in the formula. However, in centering $y_{i}'$, we use the estimator $\bar{y}$, an additional parameter. We end up saving no degrees of freedom when centering and scaling in this way. 

#### 2.2

a. The ensemble model takes the form of a linear model:
$$ \sum_{j=1}^Jw_{j}\beta_{0j}+\sum_{j=1}^J\left(w_{j}\sum_{k=1}^px_{k}\beta_{kj}\right) = \tilde{\beta_{0}}+\sum_{j=1}^Jx_{j}\tilde{\beta_{j}}$$
where $\tilde{\beta_{j}} = w_{j}\beta_{j}$ and $\tilde{\beta_{0}} = \sum_{j=1}^Jw_{j}\beta_{0j}$.

b. The optimal weights will be the coefficients of this new linear model, $\tilde{\beta_{j}}, j=0,...,J$. 

If we were to use the same training data to find these optimal weights/coefficients as we did to find the coefficients of each member of the ensemble model, we'd be fitting to the training data twice, which would cause over-fitting. An ensemble model with weights selected according to performance on the same training data as its component models were trained on will be too attuned to the particularities of that training data set and will not be able to generalize well. 

A possible improvement could be made by setting aside a portion of the training data to be used exclusively to find the weights for the ensemble model. 

#### 2.3

a. $$ logL(\beta, \sigma | D) = -\frac{n}{2}log(2\pi)-\frac{n}{2}log(\sigma^2)-\frac{1}{{2\sigma^2}}\sum_{i=1}^n(y_{i}-x_{i}^T\beta)^2 $$

b. To find the MLE for $\hat{\beta}$, 
$$ \frac{\partial}{\partial\beta} = -\frac{1}{\sigma^2}\sum_{i=1}^nx_{i}^T(y_{i}-x_{i}^T\beta)=0 \Rightarrow \sum_{i=1}^nx_{i}^Tx_{i}\beta = \sum_{i=1}^nx_{i}^Ty_{i} \Rightarrow \hat{\beta} = \left(\sum_{i=1}^nx_{i}^Tx_{i}\right)^{-1}\sum_{i=1}^nx_{i}^Ty_{i}$$
which equates to our familiar $\hat{\beta} = (X^TX)^{-1} X^Ty$.

To find the MLE for $\hat{\sigma^2}$, 
$$ \frac{\partial}{\partial\sigma^2} = -\frac{n}{{2\sigma^2}}+\frac{1}{{2(\sigma^2)^2}}\sum_{i=1}^n(y_{i}-x_{i}^T\beta)^2=0 \Rightarrow n\sigma^2 = \sum_{i=1}^n(y_{i}-x_{i}^T\beta)^2$$
leaving us with the familiar formula for estimated standard deviation, $\hat{\sigma}^2=\frac{1}{n}\sum_{i=1}^n(y_{i}-x_{i}^T\beta)^2$.

c. To derive a workable version of the AIC, we'll start with the formula $-2logL(\hat{M}^{MLE})+2df(M)$, replacing the "$logL(\hat{M}^{MLE})$" chunk with the answers from parts a and b. 
$$nlog(2\pi)+nlog\left(\frac{1}{n}\sum_{i=1}^n(y_{i}-x_{i}^T\beta)^2\right)-\frac{n}{\sum_{i=1}^n(y_{i}-x_{i}^T\beta)^2} \sum_{i=1}^n(y_{i}-x_{i}^T\beta)^2 +2df(M)$$
$$= n(log(2\pi)+ log\left(\frac{1}{n}\sum_{i=1}^n(y_{i}-x_{i}^T\beta)^2\right)-1)$$
We can drop constant terms and recognize $\sum_{i=1}^n(y_{i}-x_{i}^T\beta)^2$ as RSS, which leaves us with the more straightforward to solve
$$AIC = nlog\left(\frac{RSS}{n}\right)+2df(M)$$

d. Model 2, with $p$ extra parameters, will be favored by:

i) AIC if the first part of the criterion, negative loss, outweighs the second part, the penalty on the number of parameters. I.e., if the difference between the negative loss and  number-of-parameters  penalty of model 1 ($M_{1}$) is greater than that of $M_{2}$, AIC will have us go with $M_{2}$. 

$$-2logL(\hat{M_{1}})+2df(M_{1}) > -2logL(\hat{M_{2}})+2df(M_{2})$$

ii) $R^2$ always, as model 2 has more parameters and $R^2$ will be higher for models with more parameters than for models with fewer. 

iii) adjusted $R^2$ in situations similar to that in (i); when the penalty for the additional variables is outweighed by the negative loss.


#### 2.4
Let $Z$ equal the centered matrix $X$. We'll  augment $Z$ with $p$ rows to get $\tilde{Z} = {Z \choose \sqrt{\lambda}I_{p}}$ and $y$ with $p$ zeros to get $\tilde{y} = {y \choose 0}$ Using these new matrices in the OLS solution yields 
$$\left((Z, \sqrt{\lambda}I){Z \choose \sqrt{\lambda}I}\right)^{-1}(Z, \sqrt{\lambda}I){y \choose 0} $$
Carrying out some of the matrix multiplication above leaves our system augmented only by zeros, which can be eliminated. So,
$$ (\tilde{Z}^T\tilde{Z})^{-1}\tilde{Z}^T\tilde{y} = (Z^TZ + \lambda I)^{-1}Z^Ty $$
and we can see that OLS on the augmented data will give us the ridge regression estimates.

#### 2.5

a.
```{r, eval=FALSE}
ridreg < - function(X, y, lambda) {
  Z = scale(X)
  y = scale(y)
  Beta <- matrix(0, nrows = length(lambda), ncols = dim(Z)[2]) #initialize Beta: one set per lam
  #solve for the ridge solution for each lambda
  for(i in 1:length(lambda)){   
    Beta[,i] <- tr(solve(t(Z)%*%Z+lambda[i]*diag(1, dim(Z)[2]))%*%t(Z)%*%y)
  }
  as.data.frame(Beta)
  return(Beta)
  edof <- length(Beta != 0) # using the number of nonzero coeffs to estimate df
  return(edof)
}
```

b.
```{r, eval = FALSE}
ridregpred <- function(X, lamb) { 
  # the Betas will come from the model above
  B = Beta[,lambda==lamb]
  Z = scale(X)
  yhat = t(Z)%*%B # predict: (Z^T)(B)
return(yhat)
}
```

#### 2.6

a.
```{r, warning=FALSE, message = FALSE}
library(MASS)
library(glmnet)
library(readr)

realestate_test <- read_csv("~/Documents/ASL HW 2/realestate-test.csv")
realestate_train <- read_csv("~/Documents/ASL HW 2/realestate-train.csv")

y <- realestate_train$price # pull out prices from the training data - what we're trying to predict
x <- model.matrix(price~., realestate_train)  # convert categorical variables to dummy variables
newx <- model.matrix(~., realestate_test)  # and do the same to the test data
lam.seq = exp(seq(log(100),log(1e-5),length=500))  # make lots of lambdas to try out

glm.model <- glmnet(x, y, alpha=0, lambda=lam.seq)  # one function, lots of models
cv.gm <- cv.glmnet(x, y, alpha=0, lambda=lam.seq)  # the same familty of models fit using cv
 
```

b. I haven't been able to make plot() happy, but here are my good intentions.
```{r, eval = FALSE}
# 
beta <- coef(glm.model)

# i, coef. vals against rmse, should be an rmse per each of 500 lambda vals
yhats <- predict(glm.model, x, s="lambda.min")
error <- y-yhat.tr
rmse.tr <- sqrt(mean(error^2))

# ii, coef. vals against lambda
plot(beta, lam.seq)

# iii, coef vals against penalty
plot(beta, colSums((beta^2)))

# iv, coef. vals against edof
df <- glm.model$df 
plot(beta, df)  

```

c.
```{r}
# Run the model with cv & loocv. When would we use glmnet() rather than cv.glmnet()?
cv.gm <- cv.glmnet(x, y, alpha=0, lambda=lam.seq) 
loocv.gm <- cv.glmnet(x, y, alpha=0, lambda=lam.seq, nfolds = 1460)

# lambda vals for which cv error is lowest
cvl <- cv.gm$lambda.min 
lcvl <- loocv.gm$lambda.min 
print(cvl) # the best lambda values are printed
print(lcvl)

```
The optimal $\lambda$ according to GCV is 1.943, and for LOOCV, it's 1.822. This value changes each time the model is run. A better way to select $\lambda$ would be to run the model many times and take the average of the best $\lambda$ from each run. 

d. I used the general cross-validated model to make my predictions because it seems to be the most appropriate model, as it runs much faster and may be less likely to overfit. I used the $\lambda$ value for which the cross-validation error is lowest, lambda.min. 
```{r}
yhat <- predict(cv.gm, newx, s="lambda.min") # predict prices for the test data

write.csv(yhat, file = "Predictions.csv")
```

e.  My predicted RMSE is quie a bit lower than the ~45 I now know it turned out to be. To get a more accurate RMSE, I should train the model on a portion of the data and use the rest to calculate the predicted RMSE; that way I'm not testing on the data I trained on. 
```{r}
yhat.tr <- predict(cv.gm, x, s="lambda.min")
error <- y-yhat.tr
rmse.tr <- sqrt(mean(error^2))
print(rmse.tr)
```

